{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90495f21-02c3-43b6-a7af-713a2b6cfb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create an RNN based Python machine translation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8766a-8ca7-4d18-b877-a43b13c03f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load and preprocess data\n",
    "# Parsing the file to extract sentence pairs\n",
    "source_texts = []\n",
    "target_texts = []\n",
    "file_path = r\"C:\\Users\\kulla\\Downloads\\DatasetNLP\\hin.txt\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            source_texts.append(parts[0].lower())  # English text\n",
    "            target_texts.append(parts[1].lower())  # Hindi text\n",
    "\n",
    "# Step 2: Tokenization using Keras Tokenizer\n",
    "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=False)  # Word-level tokenization\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=False)\n",
    "\n",
    "source_tokenizer.fit_on_texts(source_texts)\n",
    "target_tokenizer.fit_on_texts(target_texts)\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "source_sequences = source_tokenizer.texts_to_sequences(source_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "# Vocabulary sizes\n",
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "# Step 3: Pad sequences to the same length\n",
    "max_sequence_length = max(max(len(seq) for seq in source_sequences), max(len(seq) for seq in target_sequences))\n",
    "source_sequences = tf.keras.preprocessing.sequence.pad_sequences(source_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Step 4: Build the model with updated input/output dimensions\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(source_vocab_size, 64, input_length=max_sequence_length),\n",
    "    tf.keras.layers.SimpleRNN(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 5: One-hot encode the target sequences for training\n",
    "target_sequences_one_hot = np.array([tf.keras.utils.to_categorical(seq, num_classes=target_vocab_size) for seq in target_sequences])\n",
    "\n",
    "# Step 6: Train the model\n",
    "model.fit(source_sequences, target_sequences_one_hot, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fd43a-7e4f-4b7d-a03c-6119973ddbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
