{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa80aee-feb8-4627-9bc4-de2dff9a4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1:NLTK \n",
    " Perform the following task using NLTK: Tokenize and tag some text, identify named entities, display a parse tre and find the ambiguity of the sentence using parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0aa4d068-a99f-4b6b-90c2-0eef4e3c0fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal: ['This', 'is', 'a', 'sample', 'sentence,', 'showing', 'off', 'the', 'stop', 'words', 'filtration.', 'Barack', 'Obama', 'was', 'the', '44th', 'President']\n",
      "Filtered words: ['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.', 'Barack', 'Obama', '44th', 'President']\n",
      "Stemmed Words: ['run', 'ran', 'runner', 'easili', 'fairli']\n",
      "Lemmatized Words: ['running', 'ran', 'runner', 'easily', 'fairly']\n",
      "POS Tags: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), (',', ','), ('showing', 'VBG'), ('off', 'RP'), ('the', 'DT'), ('stop', 'NN'), ('words', 'NNS'), ('filtration', 'NN'), ('.', '.'), ('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('44th', 'CD'), ('President', 'NNP')]\n",
      "Named Entities: (S\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  sample/JJ\n",
      "  sentence/NN\n",
      "  ,/,\n",
      "  showing/VBG\n",
      "  off/RP\n",
      "  the/DT\n",
      "  stop/NN\n",
      "  words/NNS\n",
      "  filtration/NN\n",
      "  ./.\n",
      "  (PERSON Barack/NNP Obama/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  44th/CD\n",
      "  President/NNP)\n",
      "\n",
      "Parse Tree for 'the cat sat on the mat':\n",
      "             S                     \n",
      "      _______|_______               \n",
      "     |               VP            \n",
      "     |        _______|___           \n",
      "     |       |           PP        \n",
      "     |       |    _______|___       \n",
      "     NP      |   |           NP    \n",
      "  ___|___    |   |        ___|___   \n",
      "Det      N   V   P      Det      N \n",
      " |       |   |   |       |       |  \n",
      "the     cat sat  on     the     mat\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk, CFG\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text for processing\n",
    "text = \"This is a sample sentence, showing off the stop words filtration. Barack Obama was the 44th President \"\n",
    "\n",
    " # Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))  # Get the list of English stopwords\n",
    "print(\"Before removal:\", text.split())\n",
    "\n",
    "word_tokens = word_tokenize(text)  # Tokenizing\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]  # Filter out stopwordsprint(\"Filtered Words:\", filtered_words)\n",
    "print(\"Filtered words:\",filtered_words)\n",
    " # Stemming\n",
    "ps = PorterStemmer()\n",
    "words_for_stemming = [\"running\", \"ran\", \"runner\", \"easily\", \"fairly\"]\n",
    "stemmed_words = [ps.stem(word) for word in words_for_stemming]  # Apply stemming\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n",
    " # Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words_for_stemming]  # Lemmatizing words\n",
    "print(\"Lemmatized Words:\", lemmatized_words) # Named Entity Recognition (NER)\n",
    "\n",
    "tokens = word_tokenize(text)  # Tokenize words\n",
    "\n",
    "pos_tags = pos_tag(tokens)  # Perform POS tagging\n",
    "\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "entities = ne_chunk(pos_tags)  # Perform Named Entity Recognition\n",
    "print(\"Named Entities:\", entities)\n",
    "\n",
    " # Define a simple CFG grammar and parse a sentence\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    " S -> NP VP\n",
    " NP -> Det N\n",
    " VP -> V PP\n",
    " PP -> P NP\n",
    " Det -> 'the'\n",
    " N -> 'cat' | 'mat'\n",
    " V -> 'sat'\n",
    " P -> 'on'\n",
    " \"\"\")\n",
    "parser = nltk.ChartParser(grammar)  # Create a parser using the grammar\n",
    "sentence = \"the cat sat on the mat\".split()  # Parse the sentence\n",
    "print(\"\\nParse Tree for 'the cat sat on the mat':\")\n",
    "for tree in parser.parse(sentence):\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde89be-f60c-4f5a-8e95-b74d43b9412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2: T-test and chi-square test to check whether a given sequence of words is a collocation or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e1b3769-6f8b-4a8f-b464-e787b6bb1a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram count for ('New', 'York'): 1\n",
      "T-test for ('New', 'York'): 0.8461538461538461\n",
      "Chi-squared value for ('New', 'York') association: 4.653846153846153\n",
      "Chi-squared value for ('New', 'York') association: 5.041666666666666\n",
      "Chi-squared value for ('New', 'York') association: 5.887820512820512\n",
      "Chi-squared value for ('New', 'York') association: 5.958333333333332\n"
     ]
    }
   ],
   "source": [
    " import nltk\n",
    " from nltk import bigrams\n",
    " from collections import Counter\n",
    " import math\n",
    " import pandas as pd\n",
    " # Example data (you can modify this dataset as needed)\n",
    " data = {'text': [\"I visited New York\", \"It was a New great trip\", \"I love traveling\"]}\n",
    " df = pd.DataFrame(data)\n",
    " # Step 1: Combine all text data into a single list of words\n",
    " words = (' '.join(df['text'].tolist())).split()\n",
    " # Step 2: Compute bigrams\n",
    " bigram_d = Counter(bigrams(words))\n",
    " print(f\"Bigram count for ('New', 'York'):\", bigram_d[('New', 'York')])\n",
    " # Step 3: Calculate frequencies for words and bigrams\n",
    " count_dict = Counter(words)\n",
    " bigrams_list = list(bigrams(words))\n",
    " bigram_dict = Counter(bigrams_list)\n",
    " word1 = 'New'\n",
    " word2 = 'York'\n",
    " w1,w2=word1,word2\n",
    " # Frequency of the words and the bigram\n",
    " f_w1 = count_dict[word1]\n",
    " f_w2 = count_dict[word2]\n",
    " f_w1w2 = bigram_dict[(word1, word2)]\n",
    " # Total number of words\n",
    " n = len(words)\n",
    " # Step 4: Calculate t-test value\n",
    " num = (f_w1w2 / n) - (f_w1 * f_w2 / (n * n))\n",
    " den = math.sqrt((f_w1w2 / n) / n)\n",
    " t_test = num / den\n",
    " print(f\"T-test for ('New', 'York'): {t_test}\")\n",
    " # Step 5: Create a contingency table (2x2) for chi-squared test\n",
    " a = bigram_dict[(word1, word2)]  # Count of ('New', 'York')\n",
    " b = count_dict[word1] - a  # Count of words starting with 'New' but not 'York'\n",
    " c = count_dict[word2] - a  # Count of words starting with 'York' but not 'New'\n",
    " d = sum(count_dict.values()) - (a + b + c)  # Total words minus the counts for a, b, c\n",
    " # Step 6: Calculate expected values for chi-squared test\n",
    " E_a = (a + b) * (a + c) / (a + b + c + d)\n",
    " E_b = (a + b) * (b + d) / (a + b + c + d)\n",
    " E_c = (a + c) * (c + d) / (a + b + c + d)\n",
    " E_d = (c + d) * (b + d) / (a + b + c + d)\n",
    " # Step 7: Calculate chi-squared statistic\n",
    " l1 = [a, b, c, d\n",
    " l2 = [E_a, E_b, E_c, E_d]\n",
    " chi2 = 0\n",
    " for i in range(4):\n",
    "      chi2 += (l1[i] - l2[i]) ** 2 / l2[i]\n",
    "      print(f\"Chi-squared value for ('New', 'York') association: {chi2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb13748-d450-4f94-baa7-d3c7605a3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3I:mplement decision rule-based Naïve Bayes disambiguation method to find the sense of an\n",
    "ambiguous word with the given training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3dc3d45-dc93-4377-b9cb-859e2a31f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prior Probabilities:\n",
      "  P(financial_institution) = 0.4286\n",
      "  P(shore) = 0.5714\n",
      "\n",
      " Conditional Probabilities with Laplace Smoothing:\n",
      "\n",
      "🔸 For class 'financial_institution':\n",
      "  P(need|financial_institution) = 0.0800\n",
      "  P(along|financial_institution) = 0.0400\n",
      "  P(full|financial_institution) = 0.0400\n",
      "  P(fish|financial_institution) = 0.0400\n",
      "  P(walked|financial_institution) = 0.0400\n",
      "  P(deposit|financial_institution) = 0.0800\n",
      "  P(went|financial_institution) = 0.0800\n",
      "  P(bank|financial_institution) = 0.1600\n",
      "  P(money|financial_institution) = 0.1200\n",
      "  P(fished|financial_institution) = 0.0400\n",
      "  P(river|financial_institution) = 0.0400\n",
      "  P(camped|financial_institution) = 0.0400\n",
      "  P(sat|financial_institution) = 0.0400\n",
      "  P(savings|financial_institution) = 0.0800\n",
      "  P(manager|financial_institution) = 0.0800\n",
      "\n",
      "🔸 For class 'shore':\n",
      "  P(need|shore) = 0.0345\n",
      "  P(along|shore) = 0.0690\n",
      "  P(full|shore) = 0.0690\n",
      "  P(fish|shore) = 0.0690\n",
      "  P(walked|shore) = 0.0690\n",
      "  P(deposit|shore) = 0.0345\n",
      "  P(went|shore) = 0.0345\n",
      "  P(bank|shore) = 0.1724\n",
      "  P(money|shore) = 0.0345\n",
      "  P(fished|shore) = 0.0690\n",
      "  P(river|shore) = 0.1379\n",
      "  P(camped|shore) = 0.0690\n",
      "  P(sat|shore) = 0.0690\n",
      "  P(savings|shore) = 0.0345\n",
      "  P(manager|shore) = 0.0345\n",
      "\n",
      "Prediction:\n",
      "  → Predicted Sense: financial_institution\n",
      "  → Log Probability: -16.7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Training data: \"bank\" used in two senses\n",
    "training_data = [\n",
    "    (\"I need to savings money in the bank\", \"financial_institution\"),\n",
    "    (\"I went to the bank to deposit money\", \"financial_institution\"),\n",
    "    (\"They sat on the bank and fished\", \"shore\"),\n",
    "    (\"We walked along the bank of the river\", \"shore\"),\n",
    "    (\"The river bank was full of fish\", \"shore\"),\n",
    "    (\"He is a well-known bank manager\", \"financial_institution\"),\n",
    "    (\"We camped on the bank of the river\", \"shore\"),\n",
    "]\n",
    "\n",
    "# Preprocessing function: remove stopwords, punctuation, lowercase\n",
    "def preprocess(sent):\n",
    "    sent = sent.replace('.', '')\n",
    "    tokens = word_tokenize(sent)\n",
    "    sw = set(stopwords.words('english'))\n",
    "    data = [t.lower() for t in tokens if t.isalpha() and t.lower() not in sw]\n",
    "    return data\n",
    "\n",
    "# Function to get prior and conditional probabilities\n",
    "def get_counts(training_data):\n",
    "    class_counts = Counter()\n",
    "    word_counts = {}\n",
    "    total_words = defaultdict(int)\n",
    "\n",
    "    for data, sense in training_data:\n",
    "        if sense not in word_counts:\n",
    "            word_counts[sense] = Counter()\n",
    "\n",
    "    for data, sense in training_data:\n",
    "        class_counts[sense] += 1\n",
    "        words = preprocess(data)\n",
    "        word_counts[sense].update(words)\n",
    "        total_words[sense] += len(words)\n",
    "\n",
    "    return class_counts, word_counts, total_words\n",
    "\n",
    "# Calculate probabilities and print them\n",
    "def print_probabilities(class_counts, word_counts, total_words):\n",
    "    total_sentences = sum(class_counts.values())\n",
    "    print(\" Prior Probabilities:\")\n",
    "    for sense in class_counts:\n",
    "        prior = class_counts[sense] / total_sentences\n",
    "        print(f\"  P({sense}) = {prior:.4f}\")\n",
    "\n",
    "    print(\"\\n Conditional Probabilities with Laplace Smoothing:\")\n",
    "    vocabulary = set()\n",
    "    for wc in word_counts.values():\n",
    "        vocabulary.update(wc.keys())\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    for sense in word_counts:\n",
    "        print(f\"\\n🔸 For class '{sense}':\")\n",
    "        for word in vocabulary:\n",
    "            prob = (word_counts[sense][word] + 1) / (total_words[sense] + V)\n",
    "            print(f\"  P({word}|{sense}) = {prob:.4f}\")\n",
    "\n",
    "# Test sentence classification\n",
    "def testing(test, class_counts, word_counts, total_words):\n",
    "    test_words = preprocess(test)\n",
    "    best_sense = None\n",
    "    max_prob = -np.inf\n",
    "    vocabulary = set()\n",
    "    for wc in word_counts.values():\n",
    "        vocabulary.update(wc.keys())\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    for sense in class_counts:\n",
    "        sense_prob = np.log2(class_counts[sense] / sum(class_counts.values()))\n",
    "        words_prob = 0\n",
    "        for word in test_words:\n",
    "            word_prob = (word_counts[sense][word] + 1) / (total_words[sense] + V)\n",
    "            words_prob += np.log2(word_prob)\n",
    "        total_prob = sense_prob + words_prob\n",
    "        if total_prob > max_prob:\n",
    "            max_prob = total_prob\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense, max_prob\n",
    "\n",
    "# Run the model\n",
    "class_counts, word_counts, total_words = get_counts(training_data)\n",
    "\n",
    "# Print all probabilities\n",
    "print_probabilities(class_counts, word_counts, total_words)\n",
    "\n",
    "# Classify test sentence\n",
    "test = \"She opened a savings account at the bank\"\n",
    "print(\"\\nPrediction:\")\n",
    "result, prob = testing(test, class_counts, word_counts, total_words)\n",
    "print(f\"  → Predicted Sense: {result}\\n  → Log Probability: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35a85936-8f8e-42e4-83ab-3ac4cb4c4b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prior Probabilities:\n",
      "  P(financial_institution) = 0.4286\n",
      "  P(shore) = 0.5714\n",
      "\n",
      " Conditional Probabilities with Laplace Smoothing:\n",
      "\n",
      "🔸 For class 'financial_institution':\n",
      "  P(need|financial_institution) = 0.0800\n",
      "  P(along|financial_institution) = 0.0400\n",
      "  P(full|financial_institution) = 0.0400\n",
      "  P(fish|financial_institution) = 0.0400\n",
      "  P(walked|financial_institution) = 0.0400\n",
      "  P(deposit|financial_institution) = 0.0800\n",
      "  P(went|financial_institution) = 0.0800\n",
      "  P(bank|financial_institution) = 0.1600\n",
      "  P(money|financial_institution) = 0.1200\n",
      "  P(fished|financial_institution) = 0.0400\n",
      "  P(river|financial_institution) = 0.0400\n",
      "  P(camped|financial_institution) = 0.0400\n",
      "  P(sat|financial_institution) = 0.0400\n",
      "  P(savings|financial_institution) = 0.0800\n",
      "  P(manager|financial_institution) = 0.0800\n",
      "\n",
      "🔸 For class 'shore':\n",
      "  P(need|shore) = 0.0345\n",
      "  P(along|shore) = 0.0690\n",
      "  P(full|shore) = 0.0690\n",
      "  P(fish|shore) = 0.0690\n",
      "  P(walked|shore) = 0.0690\n",
      "  P(deposit|shore) = 0.0345\n",
      "  P(went|shore) = 0.0345\n",
      "  P(bank|shore) = 0.1724\n",
      "  P(money|shore) = 0.0345\n",
      "  P(fished|shore) = 0.0690\n",
      "  P(river|shore) = 0.1379\n",
      "  P(camped|shore) = 0.0690\n",
      "  P(sat|shore) = 0.0690\n",
      "  P(savings|shore) = 0.0345\n",
      "  P(manager|shore) = 0.0345\n",
      "\n",
      "Prediction:\n",
      "  → Predicted Sense: financial_institution\n",
      "  → Log Probability: -16.7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Training data: \"bank\" used in two senses\n",
    "training_data = [\n",
    "    (\"I need to savings money in the bank\", \"financial_institution\"),\n",
    "    (\"I went to the bank to deposit money\", \"financial_institution\"),\n",
    "    (\"They sat on the bank and fished\", \"shore\"),\n",
    "    (\"We walked along the bank of the river\", \"shore\"),\n",
    "    (\"The river bank was full of fish\", \"shore\"),\n",
    "    (\"He is a well-known bank manager\", \"financial_institution\"),\n",
    "    (\"We camped on the bank of the river\", \"shore\"),\n",
    "]\n",
    "\n",
    "# Preprocessing function: remove stopwords, punctuation, lowercase\n",
    "def preprocess(sent):\n",
    "    sent = sent.replace('.', '')\n",
    "    tokens = word_tokenize(sent)\n",
    "    sw = set(stopwords.words('english'))\n",
    "    data = [t.lower() for t in tokens if t.isalpha() and t.lower() not in sw]\n",
    "    return data\n",
    "\n",
    "# Function to get prior and conditional probabilities\n",
    "def get_counts(training_data):\n",
    "    class_counts = Counter()\n",
    "    word_counts = {}\n",
    "    total_words = defaultdict(int)\n",
    "\n",
    "    for data, sense in training_data:\n",
    "        if sense not in word_counts:\n",
    "            word_counts[sense] = Counter()\n",
    "\n",
    "    for data, sense in training_data:\n",
    "        class_counts[sense] += 1\n",
    "        words = preprocess(data)\n",
    "        word_counts[sense].update(words)\n",
    "        total_words[sense] += len(words)\n",
    "\n",
    "    return class_counts, word_counts, total_words\n",
    "\n",
    "# Calculate probabilities and print them\n",
    "def print_probabilities(class_counts, word_counts, total_words):\n",
    "    total_sentences = sum(class_counts.values())\n",
    "    print(\" Prior Probabilities:\")\n",
    "    for sense in class_counts:\n",
    "        prior = class_counts[sense] / total_sentences\n",
    "        print(f\"  P({sense}) = {prior:.4f}\")\n",
    "\n",
    "    print(\"\\n Conditional Probabilities with Laplace Smoothing:\")\n",
    "    vocabulary = set()\n",
    "    for wc in word_counts.values():\n",
    "        vocabulary.update(wc.keys())\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    for sense in word_counts:\n",
    "        print(f\"\\n🔸 For class '{sense}':\")\n",
    "        for word in vocabulary:\n",
    "            prob = (word_counts[sense][word] + 1) / (total_words[sense] + V)\n",
    "            print(f\"  P({word}|{sense}) = {prob:.4f}\")\n",
    "\n",
    "# Test sentence classification\n",
    "def testing(test, class_counts, word_counts, total_words):\n",
    "    test_words = preprocess(test)\n",
    "    best_sense = None\n",
    "    max_prob = -np.inf\n",
    "    vocabulary = set()\n",
    "    for wc in word_counts.values():\n",
    "        vocabulary.update(wc.keys())\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    for sense in class_counts:\n",
    "        sense_prob = np.log2(class_counts[sense] / sum(class_counts.values()))\n",
    "        words_prob = 0\n",
    "        for word in test_words:\n",
    "            word_prob = (word_counts[sense][word] + 1) / (total_words[sense] + V)\n",
    "            words_prob += np.log2(word_prob)\n",
    "        total_prob = sense_prob + words_prob\n",
    "        if total_prob > max_prob:\n",
    "            max_prob = total_prob\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense, max_prob\n",
    "\n",
    "# Run the model\n",
    "class_counts, word_counts, total_words = get_counts(training_data)\n",
    "\n",
    "# Print all probabilities\n",
    "print_probabilities(class_counts, word_counts, total_words)\n",
    "\n",
    "# Classify test sentence\n",
    "test = \"She opened a savings account at the bank\"\n",
    "print(\"\\nPrediction:\")\n",
    "result, prob = testing(test, class_counts, word_counts, total_words)\n",
    "print(f\"  → Predicted Sense: {result}\\n  → Log Probability: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867424b-4b67-4356-9a24-0bd65ca0f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex4\n",
    "Implement the Hindle and Rooth algorithm for solving the attachment ambiguity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "089b2d05-9cec-4b84-bff2-b06bcb0bfa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the Noun:  cat\n",
      "Enter the Verb:  went\n",
      "Enter the Preposition:  with\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Preposition is attached with Noun.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#with open('corpus.txt', 'r') as file:\n",
    " #   corpus = file.read()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = [\n",
    "    \"Saw the phone with me.\",\n",
    "    \"Went to the meeting yesterday\",\n",
    "    \"Told the man to wait.\",\n",
    "    \"Gave the book to her\",\n",
    "    \"Saw the cat with her.\"\n",
    "]\n",
    "\n",
    "tokens = []\n",
    "for sentence in corpus:\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    tokens.extend([word for word in words if word not in stop_words and word not in string.punctuation])\n",
    "\n",
    "element_counts = {}\n",
    "for element in tokens:\n",
    "    element_counts[element] = element_counts.get(element, 0) + 1\n",
    "\n",
    "\n",
    "bi_grams = list(bigrams(tokens))\n",
    "\n",
    "bigram_counts = {}\n",
    "for bigram in bi_grams:\n",
    "    bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
    "\n",
    "\n",
    "noun = input(\"Enter the Noun: \")\n",
    "verb = input(\"Enter the Verb: \")\n",
    "prep = input(\"Enter the Preposition: \")\n",
    "\n",
    "n = element_counts.get(noun, 0)\n",
    "v = element_counts.get(verb, 0)\n",
    "p_n = bigram_counts.get((prep, noun), 0)\n",
    "p_v = bigram_counts.get((prep, verb), 0)\n",
    "\n",
    "def cal_prob(p_v, p_n, v, n):\n",
    "    prob_v = p_v / v if v > 0 else 0\n",
    "    prob_n = p_n / n if n > 0 else 0\n",
    "    return prob_v, prob_n\n",
    "\n",
    "prob_v, prob_n = cal_prob(p_v, p_n, v, n)\n",
    "\n",
    "def cal_lam(prob_v, prob_n):\n",
    "    if prob_n > 0 and prob_v * (1 - prob_n) > 0:\n",
    "        _lambda = math.log((prob_v * (1 - prob_n)) / prob_n, 2)\n",
    "    else:\n",
    "        _lambda = float('inf') if prob_v > 0 else float('-inf')\n",
    "    return _lambda\n",
    "\n",
    "_lambda = cal_lam(prob_v, prob_n)\n",
    "\n",
    "if _lambda > 0:\n",
    "    print(\"The Preposition is attached with Verb.\")\n",
    "elif _lambda < 0:\n",
    "    print(\"The Preposition is attached with Noun.\")\n",
    "else:\n",
    "    print(\"The Preposition attachment is cannot be determined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a6d172-5afd-4635-b44e-5c401a7ddf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex5:\n",
    "Implement forward and backward procedures using Hidden Markov Model to find the\n",
    "probability of a word sequence given a hidden Markov model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b045c85d-c706-4586-b7be-73f2b61d1709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final probability for given words of fpa :  0.0315\n",
      "final probability for given words of bpa :  0.0315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag,word_tokenize\n",
    "import pandas as pd\n",
    "import math\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def fpa(pi,state,emission,words):\n",
    "    alpha=[pi]\n",
    "    for t in range(len(words)):\n",
    "        temp=[]\n",
    "        ok=words[t]-1\n",
    "        for j in range(len(state)):\n",
    "            val=0\n",
    "            for i in range(len(state)):\n",
    "                val+=state[i][j]*emission[i][ok]*alpha[t][i]\n",
    "            temp.append(val)\n",
    "        alpha.append(temp)\n",
    "    return sum(alpha[-1])\n",
    "pi=[1,0]\n",
    "words=[3,2,1]\n",
    "emission=[[0.6,0.1,0.3],[0.1,0.7,0.2]]\n",
    "state=[[0.7,0.3],[0.5,0.5]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bpa(pi,state,emission,words):\n",
    "    beta={}\n",
    "    for i in range(len(state)):\n",
    "        beta[i]={len(words)+1:1}\n",
    "    for t in reversed(range(len(words))):\n",
    "        ok=words[t]-1\n",
    "        for i in range(len(state)):\n",
    "            val=0\n",
    "            for j in range(len(state)):\n",
    "                val+=state[i][j]*emission[i][ok]*beta[j][t+2]\n",
    "            beta[i][t+1]=val\n",
    "    fsum=0\n",
    "    for i in range(len(state)):\n",
    "        fsum+=pi[i]*beta[i][1]\n",
    "    return fsum\n",
    "ans1 = fpa(pi,state,emission,words)\n",
    "ans2 = bpa(pi, state, emission, words)\n",
    "print(\"final probability for given words of fpa : \",ans1)\n",
    "print(\"final probability for given words of bpa : \",ans2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43272d9f-6b28-427a-b0e1-021a522ae268",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex6:\n",
    "Implement Viterbi algorithm to find the probability of a word sequence, and the best tag\n",
    "sequence using Hidden Markov Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b4993dbe-ea66-489f-9154-55cdaa4327d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of best sequence: 0.010499999999999999\n",
      "Best sequence of states: s1 s2 s2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\adisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import math\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def viterbi(pi, transition, emission, words):\n",
    "    alpha = [(pi, [[] for _ in pi])]  \n",
    "\n",
    "    for t in range(len(words)):\n",
    "        temp = []\n",
    "        backtrace = []\n",
    "        word_index = words[t] - 1  \n",
    "\n",
    "        for j in range(len(transition[0])):  \n",
    "            probs = []\n",
    "            for i in range(len(transition)):  \n",
    "                calc = transition[i][j] * emission[i][word_index] * alpha[t][0][i]\n",
    "                probs.append(calc)\n",
    "            max_val = max(probs)\n",
    "            index = probs.index(max_val)\n",
    "            temp.append(max_val)\n",
    "            backtrace.append(alpha[t][1][index] + [index])\n",
    "\n",
    "        alpha.append((temp, backtrace))\n",
    "\n",
    "    final_probs = alpha[-1][0]\n",
    "    final_state = alpha[-1][1][final_probs.index(max(final_probs))]\n",
    "    return max(final_probs), final_state\n",
    "\n",
    "# Define inputs\n",
    "pi = [1, 0]  \n",
    "transition = [[0.1, 0.2], [0.5, 0.5]]  \n",
    "states = [\"s1\", \"s2\"]\n",
    "observations = [\"v1\", \"v2\", \"v3\"]  \n",
    "emission = [[0.3, 0.7, 0.5], [0.6, 0.7, 0.2]]  \n",
    "words = [3, 2, 1]  \n",
    "\n",
    "# Run Viterbi\n",
    "prob, seq = viterbi(pi, transition, emission, words)\n",
    "\n",
    "# Output\n",
    "print(\"Probability of best sequence:\", prob)\n",
    "print(\"Best sequence of states:\", ' '.join([states[i] for i in seq]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d94dda-8cb8-449e-a9e5-35dcb1c66757",
   "metadata": {},
   "outputs": [],
   "source": [
    "EX 7:\n",
    "Implement Probabilistic Context Free Grammar (PCFG) and find the inside probability of a\n",
    "word sequence using the CYK algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e5e1fd82-d1a3-4f71-af5a-a37fd0baa9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence \"the cat chased the dog\" is grammatically correct with a probability of 0.000486.\n",
      "The sentence \"the dog chased the cat\" is grammatically correct with a probability of 0.000486.\n",
      "The sentence \"the dog chased the bird\" is grammatically correct with a probability of 0.000486.\n",
      "The sentence \"Book read\" is not grammatically correct.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Define a Probabilistic Context-Free Grammar (PCFG)\n",
    "pcfg = nltk.PCFG.fromstring(\"\"\"\n",
    "S -> NP VP [0.9]\n",
    "S -> VP [0.1]\n",
    "VP -> V NP [0.5]\n",
    "VP -> V [0.5]\n",
    "NP -> Det N [0.3]\n",
    "NP -> N [0.7]\n",
    "N -> 'cat' [0.2]\n",
    "N -> 'book' [0.2]\n",
    "N -> 'bird' [0.2]\n",
    "N -> 'dog' [0.4]\n",
    "V -> 'read' [0.1]\n",
    "V -> 'chased' [0.6]\n",
    "V -> 'ate' [0.3]\n",
    "Det -> 'the' [0.5]\n",
    "Det -> 'a' [0.5]\n",
    "\"\"\")\n",
    "\n",
    "sentences = [\n",
    "    \"the cat chased the dog\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"the dog chased the bird\",\n",
    "    \"Book read\"\n",
    "]\n",
    "\n",
    "def cyk_parse_with_probability(pcfg, sentence):\n",
    "    n = len(sentence)\n",
    "    table = [[[] for _ in range(n)] for _ in range(n)]\n",
    "    \n",
    "    # Initialize the table with production probabilities\n",
    "    for i in range(n):\n",
    "        for prod in pcfg.productions(rhs=sentence[i]):\n",
    "            table[i][i].append((prod.lhs(), prod.prob()))\n",
    "    \n",
    "    # Fill the table for longer substrings\n",
    "    for length in range(2, n + 1):\n",
    "        for i in range(n - length + 1):\n",
    "            j = i + length - 1\n",
    "            for k in range(i, j):\n",
    "                for prod in pcfg.productions():\n",
    "                    rhs = prod.rhs()\n",
    "                    if len(rhs) != 2:\n",
    "                        continue  # Skip productions that are not binary\n",
    "                    left_symbol, right_symbol = rhs\n",
    "                    for left, left_prob in table[i][k]:\n",
    "                        if left == left_symbol:\n",
    "                            for right, right_prob in table[k + 1][j]:\n",
    "                                if right == right_symbol:\n",
    "                                    prob = left_prob * right_prob * prod.prob()\n",
    "                                    table[i][j].append((prod.lhs(), prob))\n",
    "    \n",
    "    # Calculate the total probability for the start symbol\n",
    "    total_prob = 0.0\n",
    "    for lhs, prob in table[0][n - 1]:\n",
    "        if lhs == pcfg.start():\n",
    "            total_prob += prob\n",
    "    \n",
    "    return total_prob\n",
    "\n",
    "# Parse the sentences and calculate their probabilities\n",
    "for sent in sentences:\n",
    "    probability = cyk_parse_with_probability(pcfg, sent.lower().split())\n",
    "    if probability > 0:\n",
    "        print(f'The sentence \"{sent}\" is grammatically correct with a probability of {probability:.6f}.')\n",
    "    else:\n",
    "        print(f'The sentence \"{sent}\" is not grammatically correct.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cd8979de-2e20-4880-ba23-a561912a0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Text Classification using Bag of Words and TF-IDF with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6d6ca-5e61-4c5c-9805-711e1e9b6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex:9\n",
    "#Implement word2vec model to explore the semantic similarity between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f2767-a80b-4da1-9369-ecbb03e3ef4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189baaf4-b31b-4657-81dd-a3184ed69777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a97f3-0475-43ac-82b4-19aed99b3639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539547f5-724c-42b4-bc88-c633e027afe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
